# -*- coding: utf-8 -*-
"""Sensory data on Rusty and Plain Road detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hhKC4pshGgkCrVXFsKB1mBRHJ_DK17Oy
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import  accuracy_score, f1_score, precision_score,confusion_matrix, recall_score, roc_auc_score
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier
from sklearn.svm import SVC
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn import svm # support vector machine
from sklearn.tree import DecisionTreeClassifier #Decision tree
from sklearn.naive_bayes import GaussianNB #Naive_bayes
from sklearn.neighbors import KNeighborsClassifier #K nearest neighbors
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report,confusion_matrix

#df_Aan = pd.read_csv('Aanchal ML.csv')
#df_Aman = pd.read_csv('Aman ML.csv')
df_Khu = pd.read_csv('Khushdeep ML.csv')
#df_Ruchi = pd.read_csv('Ruchi ML.csv')
#df_Aan.head()

df_Khu.isna().sum()

"""#Analysis on Aanchal ML Dataset"""

# Separating Target feature
X = df_Aan.drop(['Label 0,1'], axis=1)
y = df_Aan['Label 0,1']

quantile_transformer = sklearn.preprocessing.QuantileTransformer(random_state=0)
X_trans = quantile_transformer.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_trans, y, test_size = 0.25, random_state = 0)

# Random Forest Classifier
RF = RandomForestClassifier()
RF.fit(X_train, y_train)
y_pred = RF.predict(X_test)
score_RF = RF.score(X_test,y_test)
print('The accuracy of the Random Forest Model is', score_RF)
print(classification_report(y_test, y_pred))

# Support Vector Classifier (SVM/SVC)
from sklearn.svm import SVC
svc = SVC(gamma=0.22)
svc.fit(X_train, y_train)
y_pred = svc.predict(X_test)
score_svc = svc.score(X_test,y_test)
print('The accuracy of SVC model is', score_svc)
print(classification_report(y_test, y_pred))

# Decision Tree
DT = DecisionTreeClassifier()
DT.fit(X_train,y_train)
y_pred = DT.predict(X_test)
score_DT = DT.score(X_test,y_test)
print("The accuracy of the Decision tree model is ",score_DT)
print(classification_report(y_test, y_pred))

# K-Nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
score_knn = knn.score(X_test,y_test)
print('The accuracy of the KNN Model is',score_knn)
print(classification_report(y_test, y_pred))

# Logistic Regression
LR = LogisticRegression()
LR.fit(X_train, y_train)
y_pred = LR.predict(X_test)
score_LR = LR.score(X_test,y_test)
print('The accuracy of the Logistic Regression model is', score_LR)
print(classification_report(y_test, y_pred))

"""#GridSearchCV using SVM"""

from sklearn.model_selection import GridSearchCV
# defining parameter range 
param_grid = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1,0.22, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}

from sklearn.svm import SVC
svc = SVC()
grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
  
# fitting the model for grid search
grid.fit(X_train, y_train)

grid_predictions = grid.predict(X_test) 
grid_train=grid.predict(X_train)
# print classification report 
print(confusion_matrix(y_test,grid_predictions))
print(classification_report(y_test, grid_predictions))
print(confusion_matrix(y_train,grid_train))
print(classification_report(y_train,grid_train))



"""#Analysis on Aman ML Dataset"""

df_Aman.head()

# Separating Target feature
X_Aman = df_Aman.drop(['LABEL 0 OR 1'], axis=1)
y_Aman = df_Aman['LABEL 0 OR 1']

quantile_transformer = sklearn.preprocessing.QuantileTransformer(random_state=0)
X_Quant = quantile_transformer.fit_transform(X_Aman)

X_train, X_test, y_train, y_test = train_test_split(X_Quant, y_Aman, test_size = 0.20, random_state = 0)

# Random Forest Classifier
RF = RandomForestClassifier()
RF.fit(X_train, y_train)
y_pred = RF.predict(X_test)
score_RF = RF.score(X_test,y_test)
print('The accuracy of the Random Forest Model is', score_RF)
print(classification_report(y_test, y_pred))

# Support Vector Classifier (SVM/SVC)
from sklearn.svm import SVC
svc = SVC(gamma=0.22)
svc.fit(X_train, y_train)
y_pred = svc.predict(X_test)
score_svc = svc.score(X_test,y_test)
print('The accuracy of SVC model is', score_svc)
print(classification_report(y_test, y_pred))

# Decision Tree
DT = DecisionTreeClassifier()
DT.fit(X_train,y_train)
y_pred = DT.predict(X_test)
score_DT = DT.score(X_test,y_test)
print("The accuracy of the Decision tree model is ",score_DT)
print(classification_report(y_test, y_pred))

# K-Nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
score_knn = knn.score(X_test,y_test)
print('The accuracy of the KNN Model is',score_knn)
print(classification_report(y_test, y_pred))

# Logistic Regression
LR = LogisticRegression()
LR.fit(X_train, y_train)
y_pred = LR.predict(X_test)
score_LR = LR.score(X_test,y_test)
print('The accuracy of the Logistic Regression model is', score_LR)
print(classification_report(y_test, y_pred))

"""#GridSearchCV using SVM"""

from sklearn.model_selection import GridSearchCV
# defining parameter range 
param_grid = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1,0.22, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}

from sklearn.svm import SVC
svc = SVC()
grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
  
# fitting the model for grid search
grid.fit(X_train, y_train)

grid_predictions = grid.predict(X_test) 
grid_train=grid.predict(X_train)
# print classification report 
print(confusion_matrix(y_test,grid_predictions))
print(classification_report(y_test, grid_predictions))
print(confusion_matrix(y_train,grid_train))
print(classification_report(y_train,grid_train))



"""#Analysis on Ruchi ML Dataset"""

df_Ruchi.head()

# Separating Target feature
X_Ruchi = df_Ruchi.drop(['Label 0 or 1'], axis=1)
y_Ruchi = df_Ruchi['Label 0 or 1']

quantile_transformer = sklearn.preprocessing.QuantileTransformer(random_state=0)
X_Quan = quantile_transformer.fit_transform(X_Ruchi)

X_train, X_test, y_train, y_test = train_test_split(X_Quan, y_Ruchi, test_size = 0.20, random_state = 0)

# Random Forest Classifier
RF = RandomForestClassifier()
RF.fit(X_train, y_train)
y_pred = RF.predict(X_test)
score_RF = RF.score(X_test,y_test)
print('The accuracy of the Random Forest Model is', score_RF)
print(classification_report(y_test, y_pred))

# Support Vector Classifier (SVM/SVC)
from sklearn.svm import SVC
svc = SVC(gamma=0.22)
svc.fit(X_train, y_train)
y_pred = svc.predict(X_test)
score_svc = svc.score(X_test,y_test)
print('The accuracy of SVC model is', score_svc)
print(classification_report(y_test, y_pred))

# Decision Tree
DT = DecisionTreeClassifier()
DT.fit(X_train,y_train)
y_pred = DT.predict(X_test)
score_DT = DT.score(X_test,y_test)
print("The accuracy of the Decision tree model is ",score_DT)
print(classification_report(y_test, y_pred))

# K-Nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
score_knn = knn.score(X_test,y_test)
print('The accuracy of the KNN Model is',score_knn)
print(classification_report(y_test, y_pred))

# Logistic Regression
LR = LogisticRegression()
LR.fit(X_train, y_train)
y_pred = LR.predict(X_test)
score_LR = LR.score(X_test,y_test)
print('The accuracy of the Logistic Regression model is', score_LR)
print(classification_report(y_test, y_pred))

"""#GridSearchCV using SVM"""

from sklearn.model_selection import GridSearchCV
# defining parameter range 
param_grid = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1,0.22, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}

from sklearn.svm import SVC
svc = SVC()
grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
  
# fitting the model for grid search
grid.fit(X_train, y_train)

grid_predictions = grid.predict(X_test) 
grid_train=grid.predict(X_train)
# print classification report 
print(confusion_matrix(y_test,grid_predictions))
print(classification_report(y_test, grid_predictions))
print(confusion_matrix(y_train,grid_train))
print(classification_report(y_train,grid_train))

"""# Khushdeep Dataset Analysis """

df_Khu.head()

# Separating Target feature
X_khu = df_Khu.drop(["Target"], axis=1)
y_khu = df_Khu["Target"]

quantile_transformer = sklearn.preprocessing.QuantileTransformer(random_state=0)
X_Quantile = quantile_transformer.fit_transform(X_khu)

X_train, X_test, y_train, y_test = train_test_split(X_Quantile, y_khu, test_size = 0.20, random_state = 0)

# Random Forest Classifier
RF = RandomForestClassifier()
RF.fit(X_train, y_train)
y_pred = RF.predict(X_test)
score_RF = RF.score(X_test,y_test)
print('The accuracy of the Random Forest Model is', score_RF)
print(classification_report(y_test, y_pred))

# Support Vector Classifier (SVM/SVC)
from sklearn.svm import SVC
svc = SVC(gamma=0.22)
svc.fit(X_train, y_train)
y_pred = svc.predict(X_test)
score_svc = svc.score(X_test,y_test)
print('The accuracy of SVC model is', score_svc)
print(classification_report(y_test, y_pred))

# Decision Tree
DT = DecisionTreeClassifier()
DT.fit(X_train,y_train)
y_pred = DT.predict(X_test)
score_DT = DT.score(X_test,y_test)
print("The accuracy of the Decision tree model is ",score_DT)
print(classification_report(y_test, y_pred))

# K-Nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
score_knn = knn.score(X_test,y_test)
print('The accuracy of the KNN Model is',score_knn)
print(classification_report(y_test, y_pred))

# Logistic Regression
LR = LogisticRegression()
LR.fit(X_train, y_train)
y_pred = LR.predict(X_test)
score_LR = LR.score(X_test,y_test)
print('The accuracy of the Logistic Regression model is', score_LR)
print(classification_report(y_test, y_pred))

"""#GridSearchCV using SVM"""

from sklearn.model_selection import GridSearchCV
# defining parameter range 
param_grid = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1,0.22, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}

from sklearn.svm import SVC
svc = SVC()
grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
  
# fitting the model for grid search
grid.fit(X_train, y_train)

grid_predictions = grid.predict(X_test) 
grid_train=grid.predict(X_train)
# print classification report 
print(confusion_matrix(y_test,grid_predictions))
print(classification_report(y_test, grid_predictions))
print(confusion_matrix(y_train,grid_train))
print(classification_report(y_train,grid_train))